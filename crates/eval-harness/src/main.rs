use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use clap::Parser;
use serde::Deserialize;
use std::collections::BTreeMap;
use std::fs::File;
use std::path::PathBuf;
use tokio::process::Command;

#[derive(Deserialize, Debug)]
#[allow(dead_code)]
struct SessionRecord {
    session_id: String,
    timestamp: String,
    query: String,
    verdict: String,
    requires_manual_review: bool,
    math_status: String,
    math_alert_required: bool,
    domain_label: Option<String>,
    confidence_bucket: Option<String>,
}

#[derive(Parser, Debug)]
struct Args {
    /// Path to curated JSON file generated by data-pipeline
    #[arg(long)]
    input: PathBuf,

    /// Limit replays (default: all)
    #[arg(long)]
    limit: Option<usize>,

    /// Output directory
    #[arg(long, default_value = "data/eval/latest")]
    output_dir: PathBuf,

    /// Replay command tokens (e.g. --replay "cargo" --replay "run" ...)
    #[arg(long = "replay", default_values = ["cargo", "run", "--offline", "-p", "deepresearch-cli", "query", "--format", "json"])]
    replay_cmd: Vec<String>,
}

#[derive(Default, serde::Serialize)]
struct AggregateMetrics {
    total_sessions: usize,
    verdict_changed: usize,
    math_status_changed: usize,
    manual_review_changed: usize,
}

#[derive(serde::Serialize)]
struct SessionDelta {
    session_id: String,
    domain_label: Option<String>,
    confidence_bucket: Option<String>,
    verdict_before: String,
    verdict_after: String,
    math_status_before: String,
    math_status_after: String,
    manual_review_before: bool,
    manual_review_after: bool,
    timestamp: DateTime<Utc>,
}

#[derive(Default, serde::Serialize)]
struct BucketReport {
    overall: AggregateMetrics,
    by_bucket: BTreeMap<String, AggregateMetrics>,
}

fn load_records(path: &PathBuf) -> Result<Vec<SessionRecord>> {
    let file = File::open(path).with_context(|| format!("open {}", path.display()))?;
    let records: Vec<SessionRecord> = serde_json::from_reader(file)
        .with_context(|| format!("parse JSON records from {}", path.display()))?;
    Ok(records)
}

async fn replay_session(cmd: &[String], query: &str) -> Result<String> {
    let mut command = Command::new(&cmd[0]);
    command.args(&cmd[1..]);
    command.arg(query);
    let output = command.output().await.context("execute replay command")?;
    if !output.status.success() {
        return Err(anyhow::anyhow!(
            "replay failed: {}",
            String::from_utf8_lossy(&output.stderr)
        ));
    }
    Ok(String::from_utf8_lossy(&output.stdout).into_owned())
}

fn extract_verdict(stdout: &str) -> String {
    stdout.lines().next().unwrap_or_default().trim().to_string()
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    std::fs::create_dir_all(&args.output_dir)?;

    let records = load_records(&args.input)?;
    let limit = args.limit.unwrap_or(records.len());

    let mut metrics = AggregateMetrics::default();
    let mut bucket_metrics: BTreeMap<String, AggregateMetrics> = BTreeMap::new();
    let mut deltas = Vec::new();

    for (idx, record) in records.into_iter().enumerate() {
        if idx >= limit {
            break;
        }
        println!("Replaying {}", record.session_id);
        let stdout = replay_session(&args.replay_cmd, &record.query).await?;
        let verdict_after = extract_verdict(&stdout);

        let verdict_changed = verdict_after.trim() != record.verdict.trim();
        let math_changed = false; // TODO: parse math status from output
        let manual_after = record.requires_manual_review; // placeholder

        metrics.total_sessions += 1;
        if verdict_changed {
            metrics.verdict_changed += 1;
        }
        if math_changed {
            metrics.math_status_changed += 1;
        }
        if manual_after != record.requires_manual_review {
            metrics.manual_review_changed += 1;
        }

        let bucket_key = format!(
            "{}|{}",
            record
                .domain_label
                .clone()
                .unwrap_or_else(|| "unknown".into()),
            record
                .confidence_bucket
                .clone()
                .unwrap_or_else(|| "unknown".into())
        );
        let entry = bucket_metrics.entry(bucket_key).or_default();
        entry.total_sessions += 1;
        if verdict_changed {
            entry.verdict_changed += 1;
        }
        if math_changed {
            entry.math_status_changed += 1;
        }
        if manual_after != record.requires_manual_review {
            entry.manual_review_changed += 1;
        }

        deltas.push(SessionDelta {
            session_id: record.session_id,
            domain_label: record.domain_label,
            confidence_bucket: record.confidence_bucket,
            verdict_before: record.verdict,
            verdict_after,
            math_status_before: record.math_status.clone(),
            math_status_after: record.math_status,
            manual_review_before: record.requires_manual_review,
            manual_review_after: manual_after,
            timestamp: Utc::now(),
        });
    }

    let report_json = args.output_dir.join("report.json");
    let report = BucketReport {
        overall: metrics,
        by_bucket: bucket_metrics,
    };
    serde_json::to_writer_pretty(
        File::create(&report_json)?,
        &serde_json::json!({
            "summary": report,
            "deltas": deltas,
        }),
    )?;

    println!("Report written to {}", report_json.display());
    Ok(())
}
